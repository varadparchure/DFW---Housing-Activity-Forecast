# -*- coding: utf-8 -*-
"""DS_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IfyDV7dfBlYxoryEhJEU6zvOrZDfWAay

# Housing Activity Forecast - Dallas Fort Worth Metroplex
# Data Science

# SARIMA - Median Price & Sales
"""

#Connect Drive

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks//Data Science Colab/combined_housing+mortgage - Sheet1.csv")
df.head()

#Check for missing values

missing_values_new = df.isnull().sum()
print("Missing values in the dataset:")
print(missing_values_new)

#Convert 'Date' to datetime and set it as index

df['DATE'] = pd.to_datetime(df['DATE'])
df.set_index('DATE', inplace=True)

#Create a holdout period:

#Exclude the last four observations for the holdout period
#Create a holdout period:

PriceHold = df.iloc[:-4]

PriceHold

"""#### STL Decomposition"""

import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose

#Decompose the Median Price series with a monthly frequency (12 periods for yearly seasonality)
decomposition_price = seasonal_decompose(PriceHold['Median Price'], model='additive', period=12)

#Decompose the Sales series with a monthly frequency (12 periods for yearly seasonality)
decomposition_sales = seasonal_decompose(PriceHold['Sales'], model='additive', period=12)

#Plot the decomposed components for Median Price
plt.figure(figsize=(14, 10))
plt.subplot(4, 2, 1)
plt.plot(decomposition_price.observed, label='Observed - Median Price')
plt.legend(loc='upper left')
plt.subplot(4, 2, 3)
plt.plot(decomposition_price.trend, label='Trend - Median Price')
plt.legend(loc='upper left')
plt.subplot(4, 2, 5)
plt.plot(decomposition_price.seasonal, label='Seasonal - Median Price')
plt.legend(loc='upper left')
plt.subplot(4, 2, 7)
plt.plot(decomposition_price.resid, label='Residual - Median Price')
plt.legend(loc='upper left')

#Plot the decomposed components for Sales
plt.subplot(4, 2, 2)
plt.plot(decomposition_sales.observed, label='Observed - Sales')
plt.legend(loc='upper left')
plt.subplot(4, 2, 4)
plt.plot(decomposition_sales.trend, label='Trend - Sales')
plt.legend(loc='upper left')
plt.subplot(4, 2, 6)
plt.plot(decomposition_sales.seasonal, label='Seasonal - Sales')
plt.legend(loc='upper left')
plt.subplot(4, 2, 8)
plt.plot(decomposition_sales.resid, label='Residual - Sales')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

#Seasonality Visualization


import warnings
warnings.filterwarnings("ignore")
import seaborn as sns
import matplotlib.pyplot as plt

# ------------------------------------------ MEDIAN PRICE ------------------------------------------

#Temporarily create 'Month' and 'Year' columns for plotting
temp_df = PriceHold.copy()
temp_df['Month'] = temp_df.index.month
temp_df['Year'] = temp_df.index.year.astype(str)

#Create a seasonal plot for Median Price by Month, grouped by Year, without the full Year legend
plt.figure(figsize=(10, 6))

#Create the line plot for Median Price without the legend
sns.lineplot(x='Month', y='Median Price', hue='Year', data=temp_df, palette='tab10', legend=False)

#Label specific years at the end of their lines
for year in ['2010', '2015', '2021', '2024']:  # Select the years you want to label
    year_data = temp_df[temp_df['Year'] == year]
    plt.text(year_data['Month'].iloc[-1], year_data['Median Price'].iloc[-1], year,
             horizontalalignment='left', size='medium', color='black')

plt.title('Seasonal Plot of Median Price by Month (Grouped by Year)')
plt.xlabel('Month')
plt.ylabel('Median Price')
plt.grid(True)
plt.tight_layout()
plt.show()

# ------------------------------------------ SALES ------------------------------------------

#Create a seasonal plot for Sales by Month, grouped by Year, without the full Year legend
plt.figure(figsize=(10, 6))

#Create the line plot for Sales without the legend
sns.lineplot(x='Month', y='Sales', hue='Year', data=temp_df, palette='tab10', legend=False)

#Label specific years at the end of their lines
for year in ['2010', '2015', '2021', '2024']:  # Select the years you want to label
    year_data = temp_df[temp_df['Year'] == year]
    plt.text(year_data['Month'].iloc[-1], year_data['Sales'].iloc[-1], year,
             horizontalalignment='left', size='medium', color='black')

plt.title('Seasonal Plot of Sales by Month (Grouped by Year)')
plt.xlabel('Month')
plt.ylabel('Sales')
plt.grid(True)
plt.tight_layout()
plt.show()

#Series Check

import matplotlib.pyplot as plt
import numpy as np

#Set the figure size
plt.figure(figsize=(12, 12))

#Plot Median Price
plt.subplot(2, 1, 1)
plt.plot(PriceHold.index, PriceHold['Median Price'], label='Median Price', color='blue')
plt.title('Home Median Price Over Time')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.grid(True)
plt.legend()

#Plot Sales
plt.subplot(2, 1, 2)
plt.plot(PriceHold.index, PriceHold['Sales'], label='Sales', color='green')
plt.title('Home Sales Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Sales')
plt.grid(True)
plt.legend()

#Adjust layout
plt.tight_layout()
plt.show()

"""#### Transformed Series"""

import numpy as np
from scipy.stats import boxcox
from scipy.special import inv_boxcox
import matplotlib.pyplot as plt

#Apply Box-Cox transformation and create new columns for both 'Median Price' and 'Sales'
PriceHold['boxcox_transform_price'], lam_price = boxcox(PriceHold['Median Price'].dropna())
PriceHold['boxcox_transform_sales'], lam_sales = boxcox(PriceHold['Sales'].dropna())

#Set the figure size
plt.figure(figsize=(12, 12))

#Plot the Box-Cox transformed Median Price data
plt.subplot(2, 1, 1)
plt.plot(PriceHold.index, PriceHold['boxcox_transform_price'], label='Box-Cox Transformed Median Price', color='blue')
plt.xlabel('Date')
plt.ylabel('Box-Cox of Median Price')
plt.title('Box-Cox Transformed Home Median Price Over Time')
plt.grid(True)
plt.legend()

#Plot the Box-Cox transformed Sales data
plt.subplot(2, 1, 2)
plt.plot(PriceHold.index, PriceHold['boxcox_transform_sales'], label='Box-Cox Transformed Sales', color='green')
plt.xlabel('Date')
plt.ylabel('Box-Cox of Sales')
plt.title('Box-Cox Transformed Home Sales Over Time')
plt.grid(True)
plt.legend()

#Adjust layout
plt.tight_layout()
plt.show()

#Display the updated PriceHold DataFrame
PriceHold

"""#### ADF Test"""

from statsmodels.tsa.stattools import adfuller
import numpy as np

# Apply Box-Cox transformation to Median Price and Sales
PriceHold['boxcox_transform_price'], lam_price = boxcox(PriceHold['Median Price'].dropna())
PriceHold['boxcox_transform_sales'], lam_sales = boxcox(PriceHold['Sales'].dropna())

# Non-seasonal differencing (first-order difference) on the Box-Cox transformed series for both Median Price and Sales
PriceHold['Non_Seasonal_Diff_Boxcox_Price'] = PriceHold['boxcox_transform_price'].diff()
PriceHold['Non_Seasonal_Diff_Boxcox_Sales'] = PriceHold['boxcox_transform_sales'].diff()

# Seasonal differencing (assuming monthly data with yearly seasonality, so lag=12) on the Box-Cox transformed series
PriceHold['Seasonal_Diff_Boxcox_Price'] = PriceHold['boxcox_transform_price'].diff(12)
PriceHold['Seasonal_Diff_Boxcox_Sales'] = PriceHold['boxcox_transform_sales'].diff(12)

# Dropping NaN values created by differencing for testing
non_seasonal_diff_boxcox_price = PriceHold['Non_Seasonal_Diff_Boxcox_Price'].dropna()
seasonal_diff_boxcox_price = PriceHold['Seasonal_Diff_Boxcox_Price'].dropna()

non_seasonal_diff_boxcox_sales = PriceHold['Non_Seasonal_Diff_Boxcox_Sales'].dropna()
seasonal_diff_boxcox_sales = PriceHold['Seasonal_Diff_Boxcox_Sales'].dropna()

# ADF Test for Non-Seasonal Differencing on Box-Cox Transformed Data for Median Price
adf_non_seasonal_boxcox_price = adfuller(non_seasonal_diff_boxcox_price)
print("ADF Test for Non-Seasonal Differenced Box-Cox Transformed Series - Median Price")
print(f"ADF Statistic: {adf_non_seasonal_boxcox_price[0]}")
print(f"p-value: {adf_non_seasonal_boxcox_price[1]}")
for key, value in adf_non_seasonal_boxcox_price[4].items():
    print(f"Critical Value {key}: {value}")
print("Stationary" if adf_non_seasonal_boxcox_price[1] < 0.05 else "Non-Stationary", "\n")

# ADF Test for Seasonal Differencing on Box-Cox Transformed Data for Median Price
adf_seasonal_boxcox_price = adfuller(seasonal_diff_boxcox_price)
print("ADF Test for Seasonally Differenced Box-Cox Transformed Series - Median Price (12-month differencing)")
print(f"ADF Statistic: {adf_seasonal_boxcox_price[0]}")
print(f"p-value: {adf_seasonal_boxcox_price[1]}")
for key, value in adf_seasonal_boxcox_price[4].items():
    print(f"Critical Value {key}: {value}")
print("Stationary" if adf_seasonal_boxcox_price[1] < 0.05 else "Non-Stationary", "\n")

# ADF Test for Non-Seasonal Differencing on Box-Cox Transformed Data for Sales
adf_non_seasonal_boxcox_sales = adfuller(non_seasonal_diff_boxcox_sales)
print("ADF Test for Non-Seasonal Differenced Box-Cox Transformed Series - Sales")
print(f"ADF Statistic: {adf_non_seasonal_boxcox_sales[0]}")
print(f"p-value: {adf_non_seasonal_boxcox_sales[1]}")
for key, value in adf_non_seasonal_boxcox_sales[4].items():
    print(f"Critical Value {key}: {value}")
print("Stationary" if adf_non_seasonal_boxcox_sales[1] < 0.05 else "Non-Stationary", "\n")

# ADF Test for Seasonal Differencing on Box-Cox Transformed Data for Sales
adf_seasonal_boxcox_sales = adfuller(seasonal_diff_boxcox_sales)
print("ADF Test for Seasonally Differenced Box-Cox Transformed Series - Sales (12-month differencing)")
print(f"ADF Statistic: {adf_seasonal_boxcox_sales[0]}")
print(f"p-value: {adf_seasonal_boxcox_sales[1]}")
for key, value in adf_seasonal_boxcox_sales[4].items():
    print(f"Critical Value {key}: {value}")
print("Stationary" if adf_seasonal_boxcox_sales[1] < 0.05 else "Non-Stationary")

"""#### ACF and PACF charts"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from scipy.stats import boxcox

# Apply Box-Cox transformation to Median Price and Sales
PriceHold['boxcox_transform_price'], lam_price = boxcox(PriceHold['Median Price'].dropna())
PriceHold['boxcox_transform_sales'], lam_sales = boxcox(PriceHold['Sales'].dropna())

# Box-Cox transform and differencing for Median Price
data_boxcox_price = PriceHold['boxcox_transform_price']
data_diff_boxcox_price = data_boxcox_price.diff(4).dropna()  # Seasonal differencing with lag 4
data_diff_boxcox_price = data_diff_boxcox_price.diff().dropna()  # Additional non-seasonal differencing

# Box-Cox transform and differencing for Sales
data_boxcox_sales = PriceHold['boxcox_transform_sales']
data_diff_boxcox_sales = data_boxcox_sales.diff(4).dropna()  # Seasonal differencing with lag 4
data_diff_boxcox_sales = data_diff_boxcox_sales.diff().dropna()  # Additional non-seasonal differencing

# Plot ACF and PACF for the seasonally and non-seasonally differenced Box-Cox transformed data for Median Price
plt.figure(figsize=(14, 6))

# ACF plot for Median Price
plt.subplot(1, 2, 1)
plot_acf(data_diff_boxcox_price, lags=40, ax=plt.gca())
plt.title("ACF of Seasonally and Non-Seasonally Differenced Box-Cox Transformed Median Price")

# PACF plot for Median Price
plt.subplot(1, 2, 2)
plot_pacf(data_diff_boxcox_price, lags=40, ax=plt.gca())
plt.title("PACF of Seasonally and Non-Seasonally Differenced Box-Cox Transformed Median Price")

plt.tight_layout()
plt.show()

# Plot ACF and PACF for the seasonally and non-seasonally differenced Box-Cox transformed data for Sales
plt.figure(figsize=(14, 6))

# ACF plot for Sales
plt.subplot(1, 2, 1)
plot_acf(data_diff_boxcox_sales, lags=40, ax=plt.gca())
plt.title("ACF of Seasonally and Non-Seasonally Differenced Box-Cox Transformed Sales")

# PACF plot for Sales
plt.subplot(1, 2, 2)
plot_pacf(data_diff_boxcox_sales, lags=40, ax=plt.gca())
plt.title("PACF of Seasonally and Non-Seasonally Differenced Box-Cox Transformed Sales")

plt.tight_layout()
plt.show()

"""#### Forecast on Holdout Period"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Fit SARIMA model for Median Price
sarima_model_price = SARIMAX(PriceHold['Median Price'],
                             order=(0, 1, 2),
                             seasonal_order=(1, 1, 0, 12),
                             enforce_stationarity=False,
                             enforce_invertibility=False)

sarima_results_price = sarima_model_price.fit(disp=False)

# Forecast for the holdout period (next 4 observations)
forecast_price = sarima_results_price.get_forecast(steps=4)
forecast_price_ci = forecast_price.conf_int()

# Fit SARIMA model for Sales
sarima_model_sales = SARIMAX(PriceHold['Sales'],
                             order=(1, 1, 1),
                             seasonal_order=(1, 1, 1, 12),
                             enforce_stationarity=False,
                             enforce_invertibility=False)

sarima_results_sales = sarima_model_sales.fit(disp=False)

# Forecast for the holdout period (next 4 observations)
forecast_sales = sarima_results_sales.get_forecast(steps=4)
forecast_sales_ci = forecast_sales.conf_int()

# Filter data to show from 2018 onwards
start_date = '2018-01-01'
df_filtered = df[df.index >= start_date]

# Plot the forecast for Median Price from 2018
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Median Price'], label='Actual Median Price', color='blue')
#plt.plot(PriceHold.index[-4:], PriceHold['Median Price'][-4:], label='Actual - Median Price (Holdout)', color='blue')
#plt.plot(PriceHold.loc['2018':].index, PriceHold.loc['2018':]['Median Price'], label='Observed - Median Price', color='blue')
#plt.plot(PriceHold.index[-4:], PriceHold['Median Price'][-4:], label='Actual - Median Price (Holdout)', color='blue')
plt.plot(forecast_price.predicted_mean.index, forecast_price.predicted_mean, label='Forecast - Median Price', color='red')
plt.fill_between(forecast_price_ci.index,
                 forecast_price_ci.iloc[:, 0],
                 forecast_price_ci.iloc[:, 1], color='red', alpha=0.2, label='Forecast Confidence Interval')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.title('Forecast of Median Price from 2018')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the forecast for Sales from 2018
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Sales'], label='Actual Sales', color='green')
#plt.plot(PriceHold.index[-4:], PriceHold['Sales'][-4:], label='Actual - Sales (Holdout)', color='green')
#plt.plot(PriceHold.loc['2018':].index, PriceHold.loc['2018':]['Sales'], label='Observed - Sales', color='green')
#plt.plot(PriceHold.index[-4:], PriceHold['Sales'][-4:], label='Actual - Sales (Holdout)', color='green')
plt.plot(forecast_sales.predicted_mean.index, forecast_sales.predicted_mean, label='Forecast - Sales', color='orange')
plt.fill_between(forecast_sales_ci.index,
                 forecast_sales_ci.iloc[:, 0],
                 forecast_sales_ci.iloc[:, 1], color='orange', alpha=0.2, label='Forecast Confidence Interval')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.title('Forecast of Sales from 2018')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



# Display forecasted values for both Median Price and Sales
forecasted_median_price = forecast_price.predicted_mean
forecasted_sales = forecast_sales.predicted_mean
print("Forecasted Median Price:")
print(forecasted_median_price)
print("\nForecasted Sales:")
print(forecasted_sales)

#RMSE for trained models

from sklearn.metrics import mean_squared_error

# Calculate MSE and RMSE for Median Price
actual_price_holdout = PriceHold['Median Price'][-4:]
mse_price = mean_squared_error(actual_price_holdout, forecasted_median_price)
rmse_price = np.sqrt(mse_price)
print("\nMSE for Median Price:", mse_price)
print("RMSE for Median Price:", rmse_price)

# Calculate MSE and RMSE for Sales
actual_sales_holdout = PriceHold['Sales'][-4:]
mse_sales = mean_squared_error(actual_sales_holdout, forecasted_sales)
rmse_sales = np.sqrt(mse_sales)
print("\nMSE for Sales:", mse_sales)
print("RMSE for Sales:", rmse_sales)

"""#### Forecast into future"""

# Forecast for the additional 6 months (after holdout period)
additional_steps = 6

# Forecast for Median Price for additional period
forecast_price_future = sarima_results_price.get_forecast(steps=4 + additional_steps)
forecast_price_ci_future = forecast_price_future.conf_int()

# Forecast for Sales for additional period
forecast_sales_future = sarima_results_sales.get_forecast(steps=4 + additional_steps)
forecast_sales_ci_future = forecast_sales_future.conf_int()

# Plot the forecast for Median Price from 2018, including the 6-month future forecast
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Median Price'], label='Actual Median Price', color='blue')
plt.plot(forecast_price.predicted_mean.index, forecast_price.predicted_mean, label='Forecast - Median Price (Holdout)', color='red')
plt.fill_between(forecast_price_ci.index,
                 forecast_price_ci.iloc[:, 0],
                 forecast_price_ci.iloc[:, 1], color='red', alpha=0.2, label='Forecast Confidence Interval (Holdout)')
plt.plot(forecast_price_future.predicted_mean.index[-additional_steps:], forecast_price_future.predicted_mean[-additional_steps:],
         label='Forecast - Median Price (Future)', color='purple', linestyle='--')
plt.fill_between(forecast_price_ci_future.index[-additional_steps:],
                 forecast_price_ci_future.iloc[-additional_steps:, 0],
                 forecast_price_ci_future.iloc[-additional_steps:, 1], color='purple', alpha=0.2, label='Forecast Confidence Interval (Future)')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.title('Forecast of Median Price from 2018 (Including Future Forecast)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the forecast for Sales from 2018, including the 6-month future forecast
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Sales'], label='Actual Sales', color='green')
plt.plot(forecast_sales.predicted_mean.index, forecast_sales.predicted_mean, label='Forecast - Sales (Holdout)', color='orange')
plt.fill_between(forecast_sales_ci.index,
                 forecast_sales_ci.iloc[:, 0],
                 forecast_sales_ci.iloc[:, 1], color='orange', alpha=0.2, label='Forecast Confidence Interval (Holdout)')
plt.plot(forecast_sales_future.predicted_mean.index[-additional_steps:], forecast_sales_future.predicted_mean[-additional_steps:],
         label='Forecast - Sales (Future)', color='brown', linestyle='--')
plt.fill_between(forecast_sales_ci_future.index[-additional_steps:],
                 forecast_sales_ci_future.iloc[-additional_steps:, 0],
                 forecast_sales_ci_future.iloc[-additional_steps:, 1], color='brown', alpha=0.2, label='Forecast Confidence Interval (Future)')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.title('Forecast of Sales from 2018 (Including Future Forecast)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Display forecasted values for both Median Price and Sales, including the future forecast
print("Forecasted Median Price (including future 6 months):")
print(forecast_price_future.predicted_mean)

print("\nForecasted Sales (including future 6 months):")
print(forecast_sales_future.predicted_mean)







"""# SARIMAX | EXOGENOUS"""

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate the correlation matrix for the specified columns
correlation_matrix = PriceHold[['Median Price', 'MORTGAGE30US', 'FEDFUNDS']].corr()

# Plot the correlation heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, vmin=-1, vmax=1)

# Set the title
plt.title('Correlation Heatmap for Median Price, MORTGAGE30US, and FEDFUNDS')

# Display the plot
plt.tight_layout()
plt.show()

"""#### Forecast on Holdout Period"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.metrics import mean_squared_error

# Fit SARIMA model for Median Price with exogenous variable (MORTGAGE30US)
exog_variables = df[['MORTGAGE30US']]
sarima_model_price_exog = SARIMAX(PriceHold['Median Price'],
                                  exog=exog_variables.loc[PriceHold.index],
                                  order=(0, 1, 2),
                                  seasonal_order=(1, 1, 0, 12),
                                  enforce_stationarity=False,
                                  enforce_invertibility=False)

sarima_results_price_exog = sarima_model_price_exog.fit(disp=False)

# Forecast for the holdout period (next 4 observations) with exogenous variable
exog_forecast_price = exog_variables.loc[forecast_price.predicted_mean.index]
forecast_price_exog = sarima_results_price_exog.get_forecast(steps=4, exog=exog_forecast_price)
forecast_price_exog_ci = forecast_price_exog.conf_int()

# Fit SARIMA model for Sales with exogenous variable (MORTGAGE30US)
sarima_model_sales_exog = SARIMAX(PriceHold['Sales'],
                                  exog=exog_variables.loc[PriceHold.index],
                                  order=(1, 1, 1),
                                  seasonal_order=(1, 1, 1, 12),
                                  enforce_stationarity=False,
                                  enforce_invertibility=False)

sarima_results_sales_exog = sarima_model_sales_exog.fit(disp=False)

# Forecast for the holdout period (next 4 observations) with exogenous variable
exog_forecast_sales = exog_variables.loc[forecast_sales.predicted_mean.index]
forecast_sales_exog = sarima_results_sales_exog.get_forecast(steps=4, exog=exog_forecast_sales)
forecast_sales_exog_ci = forecast_sales_exog.conf_int()

# Plot the forecast for Median Price from 2018 with exogenous variable
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Median Price'], label='Actual Median Price', color='blue')
plt.plot(forecast_price_exog.predicted_mean.index, forecast_price_exog.predicted_mean, label='Forecast - Median Price (Exog)', color='red')
plt.fill_between(forecast_price_exog_ci.index,
                 forecast_price_exog_ci.iloc[:, 0],
                 forecast_price_exog_ci.iloc[:, 1], color='red', alpha=0.2, label='Forecast Confidence Interval')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.title('Forecast of Median Price from 2018 with Exogenous Variable (MORTGAGE30US)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Plot the forecast for Sales from 2018 with exogenous variable
plt.figure(figsize=(12, 6))
plt.plot(df_filtered.index, df_filtered['Sales'], label='Actual Sales', color='green')
plt.plot(forecast_sales_exog.predicted_mean.index, forecast_sales_exog.predicted_mean, label='Forecast - Sales (Exog)', color='orange')
plt.fill_between(forecast_sales_exog_ci.index,
                 forecast_sales_exog_ci.iloc[:, 0],
                 forecast_sales_exog_ci.iloc[:, 1], color='orange', alpha=0.2, label='Forecast Confidence Interval')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.title('Forecast of Sales from 2018 with Exogenous Variable (MORTGAGE30US)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Display forecasted values for both Median Price and Sales with exogenous variable
forecasted_median_price_exog = forecast_price_exog.predicted_mean
forecasted_sales_exog = forecast_sales_exog.predicted_mean
print("Forecasted Median Price with Exogenous Variable (MORTGAGE30US):")
print(forecasted_median_price_exog)
print("\nForecasted Sales with Exogenous Variable (MORTGAGE30US):")
print(forecasted_sales_exog)

# Calculate MSE and RMSE for Median Price with exogenous variable
mse_price_exog = mean_squared_error(actual_price_holdout, forecasted_median_price_exog)
rmse_price_exog = np.sqrt(mse_price_exog)
print("\nMSE for Median Price with Exogenous Variable (MORTGAGE30US):", mse_price_exog)
print("RMSE for Median Price with Exogenous Variable (MORTGAGE30US):", rmse_price_exog)

# Calculate MSE and RMSE for Sales with exogenous variable
mse_sales_exog = mean_squared_error(actual_sales_holdout, forecasted_sales_exog)
rmse_sales_exog = np.sqrt(mse_sales_exog)
print("\nMSE for Sales with Exogenous Variable (MORTGAGE30US):", mse_sales_exog)
print("RMSE for Sales with Exogenous Variable (MORTGAGE30US):", rmse_sales_exog)

"""# LSTM

### **Median Price**
"""

import pandas as pd

# Load the data
file_path ="/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Median_Price.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Display basic information about the dataset
print(data.head())
print(data.info())

data = data.sort_values(by='Date')

#Lets preprocess


# Visualize before scaling data
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Median Price'], label='Median Price')
plt.title('Median Price')
plt.xlabel('Date')
plt.ylabel('Scaled Value')
plt.legend()
plt.show()


import numpy as np
# Log transform the target variable
data['log_Median_Price'] = np.log(data['Median Price'])

# Normalize the target variable
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
data['scaled_log_Median_Price'] = scaler.fit_transform(data[['log_Median_Price']])

# Visualize the scaled data
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['scaled_log_Median_Price'], label='Scaled Log Median Price')
plt.title('Scaled Log Transformed Median Price')
plt.xlabel('Date')
plt.ylabel('Scaled Value')
plt.legend()
plt.show()

# Save the scaler for later use (for inverse transformation)
import joblib
joblib.dump(scaler, 'scaler.pkl')

#Prepare Sequences for LSTM


def create_sequences(data, sequence_length):
    x, y = [], []
    for i in range(sequence_length, len(data)):
        x.append(data[i-sequence_length:i])  # Input: last `sequence_length` values
        y.append(data[i])  # Output: next value
    return np.array(x), np.array(y)

# Define sequence length
sequence_length = 12

# Generate sequences
sequences = data['scaled_log_Median_Price'].values
x, y = create_sequences(sequences, sequence_length)

print(f"x shape: {x.shape}, y shape: {y.shape}")

#Split

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=False)

print(f"x_train shape: {x_train.shape}, x_test shape: {x_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")

#Build the model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Add early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
#Early stopping monitors validation loss and stops training if it doesn’t improve for 10 epochs.


# Train the model
history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=20,
    batch_size=32,
    callbacks=[early_stopping]
)

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""#### Forecast"""

#check the fit on Test data

from sklearn.metrics import mean_squared_error
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Ensure `scaler` is available from training
# It should have been fitted earlier like this:
# scaler = MinMaxScaler(feature_range=(0, 1))
# y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))


# Make predictions on the test data
predictions = model.predict(x_test)

# Rescale the predictions and the actual values back to their original scale
predictions_rescaled = scaler.inverse_transform(predictions)
y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

# Create a DataFrame to compare actual vs. predicted values
comparison_df = pd.DataFrame({
    'Actual': y_test_rescaled.flatten(),
    'Predicted': predictions_rescaled.flatten()
})

# Display the first few rows of the comparison DataFrame
print("Actual vs. Predicted Values:")
print(comparison_df.head())

# Plot actual vs. predicted prices
plt.figure(figsize=(12, 6))
plt.plot(range(len(y_test_rescaled)), y_test_rescaled, label='Actual Prices', color='blue')
plt.plot(range(len(predictions_rescaled)), predictions_rescaled, label='Predicted Prices', color='red')
plt.title('Actual vs. Predicted Prices')
plt.xlabel('Time Steps')
plt.ylabel('Median Price')
plt.legend()
plt.show()

# Calculate RMSE
rmse = np.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))

print(f"Root Mean Squared Error (RMSE): {rmse}")

"""#### Forecast into future"""

# Prepare the last sequence for prediction
sequence_length = 12
last_sequence = data['scaled_log_Median_Price'].values[-sequence_length:]  # Take the last 12 steps
last_sequence = last_sequence.reshape(1, sequence_length, 1)  # Reshape for LSTM input
print(f"Last sequence shape: {last_sequence.shape}")


# Rebuild the model explicitly for prediction
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Define the LSTM model for prediction
inference_model = Sequential()
inference_model.add(LSTM(units=50, return_sequences=True, input_shape=(12, 1)))
inference_model.add(Dropout(0.2))
inference_model.add(LSTM(units=50))
inference_model.add(Dropout(0.2))
inference_model.add(Dense(units=1))

# Compile the inference model
inference_model.compile(optimizer='adam', loss='mean_squared_error')

# Load the weights from the trained model
inference_model.set_weights(model.get_weights())

# Generate predictions
#rolling values

future_predictions = []  # Store future predictions
for step in range(6):
    print(f"Prediction step {step + 1}, input shape: {last_sequence.shape}")

    # Predict the next value
    next_prediction = inference_model.predict(last_sequence, verbose=0)
    future_predictions.append(next_prediction[0, 0])  # Store the predicted value

    # Update the sequence for the next prediction
    new_sequence = np.append(last_sequence[0, 1:, :], [[next_prediction[0, 0]]], axis=0)
    last_sequence = new_sequence.reshape(1, 12, 1)  # Reshape back for LSTM
    print(f"Updated last_sequence shape: {last_sequence.shape}")

# Rescale the predictions back to the original scale
future_predictions_rescaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
future_predictions_actual = np.exp(future_predictions_rescaled)  # Reverse log transformation

# Generate future dates
future_dates = pd.date_range(start=data['Date'].iloc[-1], periods=7, freq='MS')[1:]

# Combine future dates with predictions
future_df = pd.DataFrame({
    'Date': future_dates,
    'Predicted Median Price': future_predictions_actual.flatten()
})
future_df.set_index('Date', inplace=True)

# Plot the results
plt.figure(figsize=(12, 6))

# Plot the historical actual data
plt.plot(data['Date'], data['Median Price'], label='Actual Prices', color='blue')

# Plot the future predictions
plt.plot(future_df.index, future_df['Predicted Median Price'], label='Future Predictions (6 Months)', color='green', linestyle='--')

# Add plot details
plt.title('6-Month Forecast of Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

# Print the future predictions
print("Future Predictions for the Next 6 Months:")
print(future_df)

"""### **Sales**"""

# Load the data
file_path ="/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Sales.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Display basic information about the dataset
print(data.head())
print(data.info())

# Ensure that 'Date' is properly formatted
data = data.sort_values(by='Date')

#Lets preprocess


# Visualize the scaled data
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['Sales'], label='Sales')
plt.title('Sales')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()

import numpy as np
# Log transform the target variable
data['log_Sales'] = np.log(data['Sales'])

# Normalize the target variable
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(0, 1))
data['scaled_log_Sales'] = scaler.fit_transform(data[['log_Sales']])

# Visualize the scaled data
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(data['Date'], data['scaled_log_Sales'], label='Scaled Log Sales')
plt.title('Scaled Log Transformed Sales Price')
plt.xlabel('Date')
plt.ylabel('Scaled Value')
plt.legend()
plt.show()

# Save the scaler for later use (for inverse transformation)
import joblib
joblib.dump(scaler, 'scaler.pkl')

#Prepare Sequences for LSTM


# Function to create sequences
def create_sequences(data, sequence_length):
    x, y = [], []
    for i in range(sequence_length, len(data)):
        x.append(data[i-sequence_length:i])  # Input: last `sequence_length` values
        y.append(data[i])  # Output: next value
    return np.array(x), np.array(y)

# Define sequence length
sequence_length = 12

# Generate sequences
sequences = data['scaled_log_Sales'].values
x, y = create_sequences(sequences, sequence_length)

print(f"x shape: {x.shape}, y shape: {y.shape}")

#Split

from sklearn.preprocessing import MinMaxScaler

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=False)

print(f"x_train shape: {x_train.shape}, x_test shape: {x_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}")

#build the model


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Define the LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Add early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=20,
    batch_size=32,
    callbacks=[early_stopping]
)

# Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""#### Forecast"""

#Training with a Catch -- Seasonality!!!


from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Ensure `scaler` is available from training
# It should have been fitted earlier like this:
# scaler = MinMaxScaler(feature_range=(0, 1))
# y_train_scaled = scaler.fit_transform(y_train.reshape(-1, 1))

# Evaluate the model on the test data
test_loss = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}")

# Make predictions on the test data
predictions = model.predict(x_test)

# Rescale the predictions and the actual values back to their original scale
predictions_rescaled = scaler.inverse_transform(predictions)
y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

# Create a DataFrame to compare actual vs. predicted values
comparison_df = pd.DataFrame({
    'Actual': y_test_rescaled.flatten(),
    'Predicted': predictions_rescaled.flatten()
})

# Display the first few rows of the comparison DataFrame
print("Actual vs. Predicted Values:")
print(comparison_df.head())

# Plot actual vs. predicted prices
plt.figure(figsize=(12, 6))
plt.plot(range(len(y_test_rescaled)), y_test_rescaled, label='Actual Sales', color='blue')
plt.plot(range(len(predictions_rescaled)), predictions_rescaled, label='Predicted Sales', color='red')
plt.title('Actual vs. Predicted Sales')
plt.xlabel('Time Steps')
plt.ylabel('Sales')
plt.legend()
plt.show()

# Calculate RMSE and R²
rmse = np.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))
r2 = r2_score(y_test_rescaled, predictions_rescaled)

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

model

#Lets consider seasonality


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# Ensure the data is sorted by Date
data = data.sort_values(by='Date')

# Step 2: Extract time-based features (month and quarter)
data['month'] = data['Date'].dt.month
data['quarter'] = data['Date'].dt.quarter

# One-hot encode month and quarter
one_hot_encoder = OneHotEncoder(sparse_output=False)  # Correct argument
encoded_months = one_hot_encoder.fit_transform(data[['month']])
encoded_quarters = one_hot_encoder.fit_transform(data[['quarter']])

# Append the encoded features to the data
month_columns = [f'month_{i}' for i in range(1, 13)]
quarter_columns = [f'quarter_{i}' for i in range(1, 5)]

encoded_months_df = pd.DataFrame(encoded_months, columns=month_columns, index=data.index)
encoded_quarters_df = pd.DataFrame(encoded_quarters, columns=quarter_columns, index=data.index)

# Combine with the original data
data = pd.concat([data, encoded_months_df, encoded_quarters_df], axis=1)

# Step 3: Scale the target variable
data['log_Sales'] = np.log(data['Sales'])
scaler = MinMaxScaler(feature_range=(0, 1))
data['scaled_log_Sales'] = scaler.fit_transform(data[['log_Sales']])

# Step 4: Create sequences for LSTM
def create_sequences(data, features, target, sequence_length):
    x, y = [], []
    for i in range(sequence_length, len(data)):
        x.append(data[features].iloc[i-sequence_length:i].values)  # Use past `sequence_length` features
        y.append(data[target].iloc[i])  # Use the current target value
    return np.array(x), np.array(y)

# Define the feature columns (include one-hot encoded months and quarters)
feature_columns = ['scaled_log_Sales'] + month_columns + quarter_columns

# Define the sequence length
sequence_length = 12

# Generate sequences
x, y = create_sequences(data, feature_columns, 'scaled_log_Sales', sequence_length)

# Split into training and test sets
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, shuffle=False)

# Step 5: Build the LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Add early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train the model
history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=50,
    batch_size=32,
    callbacks=[early_stopping]
)

# Step 6: Plot training and validation loss
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Step 7: Evaluate the model on test data
test_loss = model.evaluate(x_test, y_test)
print(f"Test Loss: {test_loss}")

# Make predictions
predictions = model.predict(x_test)

# Rescale predictions and actual values back to original scale
predictions_rescaled = scaler.inverse_transform(predictions)
y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1))

# Step 8: Compare actual vs. predicted values
comparison_df = pd.DataFrame({
    'Actual': y_test_rescaled.flatten(),
    'Predicted': predictions_rescaled.flatten()
})

print("Actual vs. Predicted Values:")
print(comparison_df.head())

# Plot actual vs. predicted prices
plt.figure(figsize=(12, 6))
plt.plot(range(len(y_test_rescaled)), y_test_rescaled, label='Actual Sales', color='blue')
plt.plot(range(len(predictions_rescaled)), predictions_rescaled, label='Predicted Sales', color='red')
plt.title('Actual vs. Predicted Sales')
plt.xlabel('Time Steps')
plt.ylabel('Sales')
plt.legend()
plt.show()

# Step 9: Calculate RMSE and R²
rmse = np.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))
r2 = r2_score(y_test_rescaled, predictions_rescaled)

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

"""#### Forecast into future"""

# Prepare for future prediction

sequence_length = 12
num_features = x_train.shape[2]  # Number of features during training

# Step 1: Prepare the last sequence from the test data
last_sequence = x_test[-1]  # Take the last sequence from the test data
print(f"Last sequence shape before reshape: {last_sequence.shape}")

# Reshape for LSTM input
last_sequence = last_sequence.reshape(1, sequence_length, num_features)
print(f"Last sequence shape after reshape: {last_sequence.shape}")

# Generate predictions
future_predictions = []  # Store future predictions
for step in range(8):  # Predict for the next 8 months
    # Predict the next value
    next_prediction = model.predict(last_sequence, verbose=0)
    future_predictions.append(next_prediction[0, 0])  # Store the predicted value

    # Create a new feature row for the next time step
    new_features = np.zeros(num_features)
    new_features[0] = next_prediction[0, 0]  # Add the predicted scaled sales value

    # Update the last sequence with the new features
    last_sequence = np.append(last_sequence[:, 1:, :], new_features.reshape(1, 1, num_features), axis=1)
    print(f"Updated last_sequence shape: {last_sequence.shape}")

# Rescale predictions back to the original scale
future_predictions_rescaled = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
future_predictions_actual = np.exp(future_predictions_rescaled)  # Reverse log transformation

# Generate future dates
future_dates = pd.date_range(start=data['Date'].iloc[-1], periods=9, freq='MS')[1:]

# Combine future dates with predictions
future_df = pd.DataFrame({
    'Date': future_dates,
    'Predicted Sales': future_predictions_actual.flatten()
})
future_df.set_index('Date', inplace=True)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(data['Date'], data['Sales'], label='Actual Sales', color='blue')
plt.plot(future_df.index, future_df['Predicted Sales'], label='Future Predictions (8 Months)', color='green', linestyle='--')
plt.title('8-Month Forecast Without Month/Quarter Features')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.grid(True)
plt.show()

# Print the future predictions
print("Future Predictions for the Next 8 Months:")
print(future_df)



"""# XGBoost | NO EXOGENOUS

### **Median Price**
"""

import xgboost as xgb

# Load the data
# For Median Price

file_path = "/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Median_Price.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Ensure data is sorted by Date
data = data.sort_values(by='Date')

# Rename columns for consistency with modeling
data.rename(columns={'Date': 'ds', 'Median Price': 'y'}, inplace=True)
print(data.head())

plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Original Data')
plt.title('Original Time Series')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.show()

#1st Difference

data['first_diff'] = data['y'].diff()
data = data.dropna()  # Remove NaN values

# Plot first-differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['first_diff'], label='First Differenced Data')
plt.title('First Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Differenced Median Price')
plt.legend()
plt.show()

#2nd Difference

data['seasonal_diff'] = data['y'] - data['y'].shift(12)
data = data.dropna()

# Plot seasonally differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['seasonal_diff'], label='Seasonally Differenced Data')
plt.title('Seasonally Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Seasonally Differenced Median Price')
plt.legend()
plt.show()

"""#### ADF Test"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(data['seasonal_diff'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])
if result[1] < 0.05:
    print("The series is stationary.")
else:
    print("The series is not stationary.")

# Add lag features
for lag in range(1, 13):  # Create lags up to 12 months
    data[f'lag_{lag}'] = data['seasonal_diff'].shift(lag)

data = data.dropna()  # Remove NaN values created by lagging

# Define features and target
X = data[[f'lag_{i}' for i in range(1, 13)]].values  # Lag features
y = data['seasonal_diff'].values  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

#Train

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'max_depth': [3, 5, 7],  # Tree depth
    'n_estimators': [50, 100, 150],  # Number of trees
    'subsample': [0.6, 0.8, 1.0],  # Subsample ratio of training data
    'colsample_bytree': [0.6, 0.8, 1.0],  # Subsample ratio of features for each tree
    'gamma': [0, 1, 5],  # Minimum loss reduction for further split
    'min_child_weight': [1, 5, 10],  # Minimum sum of weights for a split
}

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for faster tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,  # Number of random parameter combinations to try
    scoring='r2',  # Optimize for R²
    cv=3,  # Use 3-fold cross-validation
    verbose=1,  # Display progress
    random_state=42,  # Ensure reproducibility
    n_jobs=-1  # Use all available CPUs
)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Use the best model found during random search
best_model = random_search.best_estimator_

# Display the best parameters
print("Best Hyperparameters:", random_search.best_params_)

# Add evaluation metric to the model parameters
best_model.set_params(early_stopping_rounds=10)

# Train the model with early stopping
best_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=True  # Log progress
)

# Make predictions using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

"""#### Forecast"""

# Reverse seasonal differencing
data['forecast'] = np.nan
data['forecast'].iloc[-len(y_test):] = y_pred
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')
plt.plot(data['ds'].iloc[-len(y_test):], data['reversed_forecast'].iloc[-len(y_test):],
         label='Forecasted Median Price', color='red', linestyle='--')
plt.title('Forecast vs. Actual Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.show()

"""#### Forecast into future"""

future_lags = X_test[-1].reshape(1, -1)  # Start with the last test sequence
future_forecast = []

for _ in range(12):  # Predict for 12 months
    next_pred = best_model.predict(future_lags)[0]
    future_forecast.append(next_pred)

    # Update lags
    future_lags = np.roll(future_lags, -1)  # Shift lags
    future_lags[0, -1] = next_pred  # Append new prediction as the latest lag

# Convert to original scale
future_forecast_rescaled = future_forecast + data['y'].iloc[-12:].values

# Generate future dates
future_dates = pd.date_range(start=data['ds'].iloc[-1], periods=13, freq='MS')[1:]

# Combine into a DataFrame
future_df = pd.DataFrame({'Date': future_dates, 'Forecasted Median Price': future_forecast_rescaled})

# Plot
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')
plt.plot(future_df['Date'], future_df['Forecasted Median Price'], label='Forecasted Median Price', color='green', linestyle='--')
plt.title('6-Month Forecast of Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

print(future_df)

"""### **Sales**"""

# Load the data
file_path ="/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Sales.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Ensure data is sorted by Date
data = data.sort_values(by='Date')

# Rename columns for consistency with modeling
data.rename(columns={'Date': 'ds', 'Sales': 'y'}, inplace=True)

print(data.head())

plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Original Data')
plt.title('Original Time Series')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.show()

#1st Difference

data['first_diff'] = data['y'].diff()
data = data.dropna()  # Remove NaN values

# Plot first-differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['first_diff'], label='First Differenced Data')
plt.title('First Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Differenced Median Price')
plt.legend()
plt.show()

#2nd Difference

data['seasonal_diff'] = data['y'] - data['y'].shift(12)
data = data.dropna()

# Plot seasonally differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['seasonal_diff'], label='Seasonally Differenced Data')
plt.title('Seasonally Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Seasonally Differenced Median Price')
plt.legend()
plt.show()

"""#### ADF Test"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(data['seasonal_diff'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])
if result[1] < 0.05:
    print("The series is stationary.")
else:
    print("The series is not stationary.")

# Add lag features
for lag in range(1, 13):  # Create lags up to 12 months
    data[f'lag_{lag}'] = data['seasonal_diff'].shift(lag)

data = data.dropna()  # Remove NaN values created by lagging

#SPLIT

# Define features and target
X = data[[f'lag_{i}' for i in range(1, 13)]].values  # Lag features
y = data['seasonal_diff'].values  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

#Train

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'max_depth': [3, 5, 7],  # Tree depth
    'n_estimators': [50, 100, 150],  # Number of trees
    'subsample': [0.6, 0.8, 1.0],  # Subsample ratio of training data
    'colsample_bytree': [0.6, 0.8, 1.0],  # Subsample ratio of features for each tree
    'gamma': [0, 1, 5],  # Minimum loss reduction for further split
    'min_child_weight': [1, 5, 10],  # Minimum sum of weights for a split
}

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for faster tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,  # Number of random parameter combinations to try
    scoring='r2',  # Optimize for R²
    cv=3,  # Use 3-fold cross-validation
    verbose=1,  # Display progress
    random_state=42,  # Ensure reproducibility
    n_jobs=-1  # Use all available CPUs
)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Use the best model found during random search
best_model = random_search.best_estimator_

# Display the best parameters
print("Best Hyperparameters:", random_search.best_params_)

# Add evaluation metric to the model parameters
best_model.set_params(early_stopping_rounds=10)

# Train the model with early stopping
best_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=True  # Log progress
)

# Make predictions using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Root Mean Squared Error (RMSE): {rmse}")
print(f"R-squared (R²): {r2}")

"""#### Forecast"""

# Reverse seasonal differencing
data['forecast'] = np.nan
data['forecast'].iloc[-len(y_test):] = y_pred
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')
plt.plot(data['ds'].iloc[-len(y_test):], data['reversed_forecast'].iloc[-len(y_test):],
         label='Forecasted Median Price', color='red', linestyle='--')
plt.title('Forecast vs. Actual Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.show()

"""#### Foreast into future"""

# Prepare for future forecasting
future_lags = X_test[-1].reshape(1, -1)  # Start with the last test sequence
future_forecast = []

for _ in range(12):  # Predict for 12 months
    next_pred = best_model.predict(future_lags)[0]
    future_forecast.append(next_pred)

    # Update lags
    future_lags = np.roll(future_lags, -1)  # Shift lags
    future_lags[0, -1] = next_pred  # Append new prediction as the latest lag

# Convert to original scale
future_forecast_rescaled = future_forecast + data['y'].iloc[-12:].values

# Generate future dates
future_dates = pd.date_range(start=data['ds'].iloc[-1], periods=13, freq='MS')[1:]

# Combine into a DataFrame
future_df = pd.DataFrame({'Date': future_dates, 'Forecasted Median Price': future_forecast_rescaled})

# Plot
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')
plt.plot(future_df['Date'], future_df['Forecasted Median Price'], label='Forecasted Median Price', color='green', linestyle='--')
plt.title('12-Month Forecast of Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

print(future_df)



"""# XGBoost | EXOGENOUS

### **Median Price**
"""

# Load the data
# For Median Price

file_path = "/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Median_Price.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Ensure data is sorted by Date
data = data.sort_values(by='Date')

# Rename columns for consistency with modeling
data.rename(columns={'Date': 'ds', 'Median Price': 'y'}, inplace=True)

print(data.head())

import seaborn as sns
import matplotlib.pyplot as plt

# Select only the numeric columns (Median Price, FEDFUNDS, MORTGAGE30US)
correlation_data = data[['y', 'FEDFUNDS', 'MORTGAGE30US']]

# Calculate correlation matrix
correlation_matrix = correlation_data.corr()

# Display correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Visualize correlation with a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()


#Since it has a stronger correlation with y compared to FEDFUNDS, it's a better candidate to include as a predictor in your XGBoost model.

#1st Diff


data['first_diff'] = data['y'].diff()
data = data.dropna()  # Remove NaN values

# Plot first-differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['first_diff'], label='First Differenced Data')
plt.title('First Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Differenced Median Price')
plt.legend()
plt.show()

#2nd Diff

data['seasonal_diff'] = data['y'] - data['y'].shift(12)
data = data.dropna()

# Plot seasonally differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['seasonal_diff'], label='Seasonally Differenced Data')
plt.title('Seasonally Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Seasonally Differenced Median Price')
plt.legend()
plt.show()

"""#### ADF Test"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(data['seasonal_diff'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])
if result[1] < 0.05:
    print("The series is stationary.")
else:
    print("The series is not stationary.")

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Create lagged features for the target variable
for i in range(1, 13):
    data[f'lag_{i}'] = data['seasonal_diff'].shift(i)

# Ensure alignment by dropping NaN values
data = data.dropna()

# Scale the exogenous variable (MORTGAGE30US)
scaler_exo = MinMaxScaler(feature_range=(0, 1))
# Scale the exogenous variable (MORTGAGE30US)
data.loc[:, 'scaled_mortgage'] = scaler_exo.fit_transform(data[['MORTGAGE30US']])

# Define features and target
lag_features = [f'lag_{i}' for i in range(1, 13)]
exo_feature = ['scaled_mortgage']
all_features = lag_features + exo_feature

X = data[all_features].values  # Combined lag and exogenous features
y = data['seasonal_diff'].values  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Display shapes
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'max_depth': [3, 5, 7],  # Tree depth
    'n_estimators': [50, 100, 150],  # Number of trees
    'subsample': [0.6, 0.8, 1.0],  # Subsample ratio of training data
    'colsample_bytree': [0.6, 0.8, 1.0],  # Subsample ratio of features for each tree
    'gamma': [0, 1, 5],  # Minimum loss reduction for further split
    'min_child_weight': [1, 5, 10],  # Minimum sum of weights for a split
}

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for faster tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,  # Number of random parameter combinations to try
    scoring='r2',  # Optimize for R²
    cv=3,  # Use 3-fold cross-validation
    verbose=1,  # Display progress
    random_state=42,  # Ensure reproducibility
    n_jobs=-1  # Use all available CPUs
)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Use the best model found during random search
best_model = random_search.best_estimator_

# Display the best parameters
print("Best Hyperparameters:", random_search.best_params_)

# Add evaluation metric to the model parameters
best_model.set_params(early_stopping_rounds=10)

# Train the model with early stopping
best_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=True  # Log progress
)

# Make predictions using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Root Mean Squared Error (RMSE): {rmse}")

# Get feature importances from the trained model
feature_importances = best_model.feature_importances_

# Verify the number of features matches
print(f"Number of features in the model: {len(feature_importances)}")
print(f"Number of features in X_train: {X_train.shape[1]}")

# Ensure the feature names match the number of features in X_train
feature_names = [f'lag_{i}' for i in range(1, 13)] + [f'mortgage_lag_{i}' for i in range(1, X_train.shape[1] - 12 + 1)]

# Check if the lengths match
if len(feature_names) != len(feature_importances):
    raise ValueError(f"Feature names length ({len(feature_names)}) does not match feature importances length ({len(feature_importances)})")

# Create a DataFrame for feature importances
import pandas as pd
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Display the feature importances
print("Feature Importances:")
print(importance_df)

# Plot feature importances
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.title('Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

"""#### Forecast"""

# Reverse seasonal differencing
data['forecast'] = np.nan  # Initialize the forecast column with NaN
data.iloc[-len(y_test):, data.columns.get_loc('forecast')] = y_pred  # Fill forecast for the test data range
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)  # Reverse the seasonal differencing

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')
plt.plot(data['ds'].iloc[-len(y_test):], data['reversed_forecast'].iloc[-len(y_test):],
         label='Forecasted Median Price', color='red', linestyle='--')
plt.title('Forecast vs. Actual Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

# Print the forecasted values
forecast_df = data[['ds', 'y', 'forecast', 'reversed_forecast']].iloc[-len(y_test):]
print("Forecasted vs Actual Data:")
print(forecast_df)

#Keep the Important Features -- (feature importances)


from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# Ensure lagged features for 'scaled_mortgage' are created
for i in range(1, 4):  # Create 3 lagged features for 'scaled_mortgage'
    data[f'mortgage_lag_{i}'] = data['scaled_mortgage'].shift(i)

# Ensure alignment by dropping NaN values
data = data.dropna().reset_index(drop=True)

# Define important features based on feature importances
important_features = [
    f'lag_{i}' for i in range(1, 8)
] + ['mortgage_lag_1']  # Keep only the most important lags and exogenous feature

# Define X and y with reduced feature set
X_reduced = data[important_features].values
y = data['seasonal_diff'].values

# Train-test split
X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42, shuffle=False)

# Display shapes
print(f"X_train_reduced shape: {X_train_reduced.shape}, y_train shape: {y_train.shape}")
print(f"X_test_reduced shape: {X_test_reduced.shape}, y_test shape: {y_test.shape}")

# Define parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 150],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'min_child_weight': [1, 5, 10],
}

# Initialize XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,
    scoring='r2',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit RandomizedSearchCV
random_search.fit(X_train_reduced, y_train)

# Use the best model found during random search
best_model_reduced = random_search.best_estimator_
print("Best Parameters:", random_search.best_params_)

# Train the best model with early stopping
best_model_reduced.fit(
    X_train_reduced, y_train,
    eval_set=[(X_test_reduced, y_test)],
    verbose=True  # Log progress
)

# Make predictions
y_pred_reduced = best_model_reduced.predict(X_test_reduced)

# Evaluate the model
rmse_reduced = np.sqrt(mean_squared_error(y_test, y_pred_reduced))

print(f"Root Mean Squared Error (RMSE) with Reduced Features: {rmse_reduced}")

print("")
print("")

# Compare RMSE
if rmse_reduced < rmse:
    print("Model performance improved by removing less important features!")
else:
    print("Model performance did not improve by removing less important features.")

print("")
print("")

# Display the features kept in the reduced feature set
print("Kept Features in the Reduced Feature Set:")
for feature in important_features:
    print(feature)

# Reverse seasonal differencing consistently
data['forecast'] = np.nan  # Initialize the forecast column with NaN
data.iloc[-len(y_test):, data.columns.get_loc('forecast')] = y_pred_reduced  # Fill forecast for the test data range
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)  # Reverse seasonal differencing for the forecast

# Plot the results across the full dataset
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')  # Plot the full actual data
plt.plot(data['ds'], data['reversed_forecast'], label='Forecasted Median Price', color='red', linestyle='--')  # Overlay forecast

# Add title and labels for consistency
plt.title('Forecast vs. Actual Median Price (Aligned Timeline)')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

# Display forecasted vs actual values for the test period
forecast_df = data[['ds', 'y', 'forecast', 'reversed_forecast']].iloc[-len(y_test):]
print("Forecasted vs Actual Data:")
print(forecast_df)

"""#### Forecast into future"""

# Prepare for future prediction
future_lags = X_reduced[-1].reshape(1, -1)  # Start with the last reduced feature set
future_forecast = []

for _ in range(6):  # Predict for 6 months
    next_pred = best_model_reduced.predict(future_lags)[0]  # Predict the next value
    future_forecast.append(next_pred)  # Store the prediction

    # Update lag features
    new_lags = np.roll(future_lags, -1)  # Shift lags to the left
    new_lags[0, -len(important_features):] = next_pred  # Append the new prediction as the most recent lag
    future_lags = new_lags.reshape(1, -1)  # Reshape for the next iteration

# Reverse seasonal differencing for the forecast
future_forecast_rescaled = [
    future_forecast[i] + data['y'].iloc[-12 + i] for i in range(len(future_forecast))
]

# Generate future dates
last_date = data['ds'].iloc[-1]
future_dates = pd.date_range(start=last_date, periods=7, freq='MS')[1:]

# Create a DataFrame for the future forecast
future_df = pd.DataFrame({
    'ds': future_dates,
    'forecast': future_forecast,
    'reversed_forecast': future_forecast_rescaled
})

# Plot future predictions along with the historical data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')  # Historical data
plt.plot(future_df['ds'], future_df['reversed_forecast'], label='Future Forecast (6 Months)', color='green', linestyle='--')  # Future forecast

# Add plot details
plt.title('6-Month Forecast of Median Price')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

# Print the future forecast
print("Future Forecast for the Next 6 Months:")
print(future_df)

"""#### Key Insights:


1.   Slight Upward Trend: The forecast shows a minor increase in prices, suggesting some recovery after the recent dip.
2.   Stabilization: The forecast appears less volatile compared to past fluctuations, indicating a more stable market.
3. Conservative Prediction: The model’s predictions show moderate growth, potentially missing some sharper trends.
4. Possible Underfitting: The model’s stability may suggest it’s not capturing all recent price changes effectively.
5. Balanced Market: The market seems to be heading towards stability, with fewer expected large movements in the short term.

Sep Actual : $389900

### **Sales**
"""

# Load the data
# For Median Price

file_path = "/content/drive/MyDrive/Colab Notebooks/Data Science Colab/Housing_Activity_Sales.csv"
data = pd.read_csv(file_path, parse_dates=['Date'])

# Ensure data is sorted by Date
data = data.sort_values(by='Date')

# Rename columns for consistency with modeling
data.rename(columns={'Date': 'ds', 'Sales': 'y'}, inplace=True)

print(data.head())

data['first_diff'] = data['y'].diff()
data = data.dropna()  # Remove NaN values

# Plot first-differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['first_diff'], label='First Differenced Data')
plt.title('First Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Differenced Median Price')
plt.legend()
plt.show()

data['seasonal_diff'] = data['y'] - data['y'].shift(12)
data = data.dropna()

# Plot seasonally differenced data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['seasonal_diff'], label='Seasonally Differenced Data')
plt.title('Seasonally Differenced Time Series')
plt.xlabel('Date')
plt.ylabel('Seasonally Differenced Median Price')
plt.legend()
plt.show()

from statsmodels.tsa.stattools import adfuller

result = adfuller(data['seasonal_diff'].dropna())
print('ADF Statistic:', result[0])
print('p-value:', result[1])
if result[1] < 0.05:
    print("The series is stationary.")
else:
    print("The series is not stationary.")

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Create lagged features for the target variable
for i in range(1, 13):
    data[f'lag_{i}'] = data['seasonal_diff'].shift(i)

# Ensure alignment by dropping NaN values
data = data.dropna()

# Scale the exogenous variable (MORTGAGE30US)
scaler_exo = MinMaxScaler(feature_range=(0, 1))
# Scale the exogenous variable (MORTGAGE30US)
data.loc[:, 'scaled_mortgage'] = scaler_exo.fit_transform(data[['MORTGAGE30US']])

# Define features and target
lag_features = [f'lag_{i}' for i in range(1, 13)]
exo_feature = ['scaled_mortgage']
all_features = lag_features + exo_feature

X = data[all_features].values  # Combined lag and exogenous features
y = data['seasonal_diff'].values  # Target variable

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# Display shapes
print(f"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np
import matplotlib.pyplot as plt

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Learning rate
    'max_depth': [3, 5, 7],  # Tree depth
    'n_estimators': [50, 100, 150],  # Number of trees
    'subsample': [0.6, 0.8, 1.0],  # Subsample ratio of training data
    'colsample_bytree': [0.6, 0.8, 1.0],  # Subsample ratio of features for each tree
    'gamma': [0, 1, 5],  # Minimum loss reduction for further split
    'min_child_weight': [1, 5, 10],  # Minimum sum of weights for a split
}

# Initialize the XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for faster tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,  # Number of random parameter combinations to try
    scoring='r2',  # Optimize for R²
    cv=3,  # Use 3-fold cross-validation
    verbose=1,  # Display progress
    random_state=42,  # Ensure reproducibility
    n_jobs=-1  # Use all available CPUs
)

# Fit RandomizedSearchCV to the training data
random_search.fit(X_train, y_train)

# Use the best model found during random search
best_model = random_search.best_estimator_

# Display the best parameters
print("Best Hyperparameters:", random_search.best_params_)

# Add evaluation metric to the model parameters
best_model.set_params(early_stopping_rounds=10)

# Train the model with early stopping
best_model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    verbose=True  # Log progress
)

# Make predictions using the best model
y_pred = best_model.predict(X_test)

# Evaluate the best model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Root Mean Squared Error (RMSE): {rmse}")

"""#### Forecast"""

# Reverse seasonal differencing
data['forecast'] = np.nan  # Initialize the forecast column with NaN
data.iloc[-len(y_test):, data.columns.get_loc('forecast')] = y_pred  # Fill forecast for the test data range
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)  # Reverse the seasonal differencing

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Sales', color='blue')
plt.plot(data['ds'].iloc[-len(y_test):], data['reversed_forecast'].iloc[-len(y_test):],
         label='Forecasted Sales', color='red', linestyle='--')
plt.title('Forecast vs. Actual Sales')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.grid(True)
plt.show()

# Print the forecasted values
forecast_df = data[['ds', 'y', 'forecast', 'reversed_forecast']].iloc[-len(y_test):]
print("Forecasted vs Actual Data:")
print(forecast_df)

#feature importances


from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


# Ensure lagged features for 'scaled_mortgage' are created
for i in range(1, 4):  # Create 3 lagged features for 'scaled_mortgage'
    data[f'mortgage_lag_{i}'] = data['scaled_mortgage'].shift(i)

# Ensure alignment by dropping NaN values
data = data.dropna().reset_index(drop=True)

# Verify if 'mortgage_lag_1' is now in the DataFrame
print(data.columns)

# Define important features based on feature importances
important_features = [
    f'lag_{i}' for i in range(1, 8)
] + ['mortgage_lag_1']  # Keep only the most important lags and exogenous feature

# Define X and y with reduced feature set
X_reduced = data[important_features].values
y = data['seasonal_diff'].values

# Train-test split
X_train_reduced, X_test_reduced, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42, shuffle=False)

# Display shapes
print(f"X_train_reduced shape: {X_train_reduced.shape}, y_train shape: {y_train.shape}")
print(f"X_test_reduced shape: {X_test_reduced.shape}, y_test shape: {y_test.shape}")

# Define parameter grid for hyperparameter tuning
param_grid = {
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'n_estimators': [50, 100, 150],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'min_child_weight': [1, 5, 10],
}

# Initialize XGBoost model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, tree_method='hist')

# Use RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid,
    n_iter=50,
    scoring='r2',
    cv=3,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# Fit RandomizedSearchCV
random_search.fit(X_train_reduced, y_train)

# Use the best model found during random search
best_model_reduced = random_search.best_estimator_
print("Best Parameters:", random_search.best_params_)

# Train the best model with early stopping
best_model_reduced.fit(
    X_train_reduced, y_train,
    eval_set=[(X_test_reduced, y_test)],
    verbose=True  # Log progress
)

# Make predictions
y_pred_reduced = best_model_reduced.predict(X_test_reduced)

# Evaluate the model
rmse_reduced = np.sqrt(mean_squared_error(y_test, y_pred_reduced))
r2_reduced = r2_score(y_test, y_pred_reduced)

print(f"Root Mean Squared Error (RMSE) with Reduced Features: {rmse_reduced}")
print(f"R-squared (R²) with Reduced Features: {r2_reduced}")

# Compare RMSE
if rmse_reduced < rmse:
    print("Model performance improved by removing less important features!")
else:
    print("Model performance did not improve by removing less important features.")

# Display the features kept in the reduced feature set
print("Kept Features in the Reduced Feature Set:")
for feature in important_features:
    print(feature)

# Reverse seasonal differencing consistently
data['forecast'] = np.nan  # Initialize the forecast column with NaN
data.iloc[-len(y_test):, data.columns.get_loc('forecast')] = y_pred_reduced  # Fill forecast for the test data range
data['reversed_forecast'] = data['forecast'] + data['y'].shift(12)  # Reverse seasonal differencing for the forecast

# Plot the results across the full dataset
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Median Price', color='blue')  # Plot the full actual data
plt.plot(data['ds'], data['reversed_forecast'], label='Forecasted Median Price', color='red', linestyle='--')  # Overlay forecast

# Add title and labels for consistency
plt.title('Forecast vs. Actual Median Price (Aligned Timeline)')
plt.xlabel('Date')
plt.ylabel('Median Price')
plt.legend()
plt.grid(True)
plt.show()

# Display forecasted vs actual values for the test period
forecast_df = data[['ds', 'y', 'forecast', 'reversed_forecast']].iloc[-len(y_test):]
print("Forecasted vs Actual Data:")
print(forecast_df)

"""#### Forecast into future"""

# Prepare for future prediction
future_lags = X_reduced[-1].reshape(1, -1)  # Start with the last reduced feature set
future_forecast = []

for _ in range(6):  # Predict for 6 months
    next_pred = best_model_reduced.predict(future_lags)[0]  # Predict the next value
    future_forecast.append(next_pred)  # Store the prediction

    # Update lag features
    new_lags = np.roll(future_lags, -1)  # Shift lags to the left
    new_lags[0, -len(important_features):] = next_pred  # Append the new prediction as the most recent lag
    future_lags = new_lags.reshape(1, -1)  # Reshape for the next iteration

# Reverse seasonal differencing for the forecast
future_forecast_rescaled = [
    future_forecast[i] + data['y'].iloc[-12 + i] for i in range(len(future_forecast))
]

# Generate future dates
last_date = data['ds'].iloc[-1]
future_dates = pd.date_range(start=last_date, periods=7, freq='MS')[1:]

# Create a DataFrame for the future forecast
future_df = pd.DataFrame({
    'ds': future_dates,
    'forecast': future_forecast,
    'reversed_forecast': future_forecast_rescaled
})

# Plot future predictions along with the historical data
plt.figure(figsize=(12, 6))
plt.plot(data['ds'], data['y'], label='Actual Sales Price', color='blue')  # Historical data
plt.plot(future_df['ds'], future_df['reversed_forecast'], label='Future Forecast (6 Months)', color='green', linestyle='--')  # Future forecast

# Add plot details
plt.title('6-Month Forecast of Sales')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.grid(True)
plt.show()

# Print the future forecast
print("Future Forecast for the Next 6 Months:")
print(future_df)

"""#### Key Insights:

1. Decrease in Forecast: The forecast initially predicts a decline in sales, reaching a lower level before showing some recovery.
2. Continued Volatility: Similar to historical data, the forecasted sales exhibit some fluctuation, suggesting that the market is expected to remain volatile in the short term.
3. Lower Levels: The forecasted values remain significantly below the previous highs, indicating potential continued weakness in sales compared to earlier periods.
4. Limited Confidence in Growth: The model does not predict a sharp recovery or return to previous peak values, suggesting limited confidence in a strong rebound.

Sep Actual 7231 - FRED
"""